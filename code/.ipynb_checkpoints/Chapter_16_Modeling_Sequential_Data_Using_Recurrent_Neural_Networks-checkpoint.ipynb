{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ada5b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb141d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurrences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:03\n",
      "Map reviews to ints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '.', ',', 'and', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:02\n"
     ]
    }
   ],
   "source": [
    "## Separate words and\n",
    "## count each word's occurrence\n",
    "from collections import Counter\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Counting words occurrences')\n",
    "for i,review in enumerate(df['review']):\n",
    "    text = ''.join([c if c not in punctuation else ' '+c+' ' for c in review]).lower()\n",
    "    df.loc[i,'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())\n",
    "## Create a mapping\n",
    "## Map each unique word to an integer\n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Map reviews to ints')\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9c4c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define same-length sequences\n",
    "## if sequence length < 200: left-pad with zeros\n",
    "## if sequence length > 200: use the last 200 elements\n",
    "sequence_length = 200 ## (Known as T in our RNN formulas)\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length),dtype=int)\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c93abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequences[:25000,:]\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = sequences[25000:,:]\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fbdea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to generate mini-batches:\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x = x[:n_batches*batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "        else:\n",
    "            yield x[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5da94693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(self):\n",
    "    ## Define the placeholders\n",
    "    tf_x = tf.placeholder(tf.int32,shape=(self.batch_size, self.seq_len),name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.float32,shape=(self.batch_size),name='tf_y')\n",
    "    tf_keepprob = tf.placeholder(tf.float32,name='tf_keepprob')\n",
    "    ## Create the embedding layer\n",
    "    embedding = tf.Variable(tf.random_uniform((self.n_words, self.embed_size),minval=-1, maxval=1),name='embedding')\n",
    "    embed_x = tf.nn.embedding_lookup(embedding, tf_x,name='embeded_x')\n",
    "    ## Define LSTM cell and stack them together\n",
    "    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(self.lstm_size),output_keep_prob=tf_keepprob) for i in range(self.num_layers)])\n",
    "    ## Define the initial state:\n",
    "    self.initial_state = cells.zero_state(self.batch_size, tf.float32) \n",
    "    print(' << initial state >> ', self.initial_state)\n",
    "    lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cells, embed_x,initial_state=self.initial_state)\n",
    "    ## [batch_size, max_time, cells.output_size]\n",
    "    print('\\n << lstm_output >> ', lstm_outputs)\n",
    "    print('\\n << final state >> ', self.final_state)\n",
    "    logits = tf.layers.dense(inputs=lstm_outputs[:, -1],units=1, activation=None,name='logits')\n",
    "    logits = tf.squeeze(logits, name='logits_squeezed')\n",
    "    print ('\\n << logits >> ', logits)\n",
    "    y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
    "    predictions = {\n",
    "    'probabilities': y_proba,\n",
    "    'labels' : tf.cast(tf.round(y_proba), tf.int32,\n",
    "    name='labels')\n",
    "    }\n",
    "    print('\\n << predictions >> ', predictions)\n",
    "    ## Define the cost function\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_y, logits=logits),name='cost')\n",
    "\n",
    "    ## Define the optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "    train_op = optimizer.minimize(cost, name='train_op')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62103cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class SentimentRNN(object):\n",
    "    def __init__(self, n_words, seq_len=200,lstm_size=256, num_layers=1, batch_size=64,learning_rate=0.0001, embed_size=200):\n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size ## number of hidden units\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a501bc74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "227a4d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, X_train, y_train, num_epochs):\n",
    "    with tf.Session(graph=self.g) as sess:\n",
    "        sess.run(self.init_op)\n",
    "        iteration = 1\n",
    "        for epoch in range(num_epochs):\n",
    "            state = sess.run(self.initial_state)\n",
    "\n",
    "            for batch_x, batch_y in create_batch_generator(X_train, y_train, self.batch_size):\n",
    "                feed = {'tf_x:0': batch_x,'tf_y:0': batch_y,'tf_keepprob:0': 0.5,self.initial_state : state}\n",
    "                loss, _, state = sess.run(['cost:0', 'train_op',self.final_state],feed_dict=feed)\n",
    "                if iteration % 20 == 0:\n",
    "                    print(\"Epoch: %d/%d Iteration: %d | Train loss: %.5f\" % (epoch + 1, num_epochs,iteration, loss))\n",
    "                iteration +=1\n",
    "            if (epoch+1)%10 == 0:\n",
    "                self.saver.save(sess,\"model/sentiment-%d.ckpt\" % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23056fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X_data, return_proba=False):\n",
    "    preds = []\n",
    "    with tf.Session(graph = self.g) as sess:\n",
    "        self.saver.restore(sess, tf.train.latest_checkpoint('./model/'))\n",
    "        test_state = sess.run(self.initial_state)\n",
    "        for ii, batch_x in enumerate(create_batch_generator(X_data, None, batch_size=self.batch_size), 1):\n",
    "            feed = {'tf_x:0' : batch_x,'tf_keepprob:0' : 1.0,self.initial_state : test_state}\n",
    "        if return_proba:\n",
    "            pred, test_state = sess.run(['probabilities:0', self.final_state],feed_dict=feed)\n",
    "        else:\n",
    "            pred, test_state = sess.run(['labels:0', self.final_state],feed_dict=feed)\n",
    "        preds.append(pred)\n",
    "\n",
    "    return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4064868",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SentimentRNN' object has no attribute 'build'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-44839a8bfb28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mn_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_int\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentimentRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membed_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-2d0b76e3b5bc>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_words, seq_len, lstm_size, num_layers, batch_size, learning_rate, embed_size)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m123\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SentimentRNN' object has no attribute 'build'"
     ]
    }
   ],
   "source": [
    "n_words = max(list(word_to_int.values())) + 1\n",
    "rnn = SentimentRNN(n_words=n_words,seq_len=sequence_length,embed_size=256,lstm_size=128,num_layers=1,batch_size=100,learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e53b0e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-87206b1a98ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rnn' is not defined"
     ]
    }
   ],
   "source": [
    "rnn.train(X_train, y_train, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30eddda8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-0da02dfb0db6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test Acc.: %.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rnn' is not defined"
     ]
    }
   ],
   "source": [
    "preds = rnn.predict(X_test)\n",
    "y_true = y_test[:len(preds)]\n",
    "print('Test Acc.: %.3f' % (np.sum(preds == y_true) / len(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae7a91bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-fee6f23fbdc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_proba\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rnn' is not defined"
     ]
    }
   ],
   "source": [
    "proba = rnn.predict(X_test, return_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d0b425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## Reading and processing text\n",
    "with open('pg2265.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()\n",
    "text = text[15858:]\n",
    "chars = set(text)\n",
    "char2int = {ch:i for i,ch in enumerate(chars)}\n",
    "int2char = dict(enumerate(chars))\n",
    "text_ints = np.array([char2int[ch] for ch in text],dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64ab4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(sequence, batch_size, num_steps):\n",
    "    tot_batch_length = batch_size * num_steps\n",
    "    num_batches = int(len(sequence) / tot_batch_length)\n",
    "    if num_batches*tot_batch_length + 1 > len(sequence):\n",
    "        num_batches = num_batches - 1\n",
    "    ## Truncate the sequence at the end to get rid of\n",
    "    ## remaining charcaters that do not make a full batch\n",
    "    x = sequence[0: num_batches*tot_batch_length]\n",
    "    y = sequence[1: num_batches*tot_batch_length + 1]\n",
    "    ## Split x & y into a list batches of sequences:\n",
    "    x_batch_splits = np.split(x, batch_size)\n",
    "    y_batch_splits = np.split(y, batch_size)\n",
    "    ## Stack the batches together\n",
    "    ## batch_size x tot_batch_length\n",
    "    x = np.stack(x_batch_splits)\n",
    "    y = np.stack(y_batch_splits)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea06e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_generator(data_x, data_y, num_steps):\n",
    "    batch_size, tot_batch_length = data_x.shape\n",
    "    num_batches = int(tot_batch_length/num_steps)\n",
    "    for b in range(num_batches):\n",
    "        yield (data_x[:, b*num_steps:(b+1)*num_steps],data_y[:, b*num_steps:(b+1)*num_steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02199e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "class CharRNN(object):\n",
    "    def __init__(self, num_classes, batch_size=64,num_steps=100, lstm_size=128,num_layers=1, learning_rate=0.001,keep_prob=0.5, grad_clip=5,sampling=False):\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build(sampling=sampling)\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "caa11ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(self, sampling):\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "    else:\n",
    "        batch_size = self.batch_size\n",
    "        num_steps = self.num_steps\n",
    "    tf_x = tf.placeholder(tf.int32,shape=[batch_size, num_steps],name='tf_x')\n",
    "    tf_y = tf.placeholder(tf.int32,shape=[batch_size, num_steps],name='tf_y')\n",
    "    tf_keepprob = tf.placeholder(tf.float32,name='tf_keepprob')\n",
    "    # One-hot encoding:\n",
    "    x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n",
    "    y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n",
    "    ### Build the multi-layer RNN cells\n",
    "    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(self.lstm_size),output_keep_prob=tf_keepprob) for _ in range(self.num_layers)])\n",
    "    ## Define the initial state\n",
    "    self.initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "    ## Run each sequence step through the RNN\n",
    "    lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cells, x_onehot,initial_state=self.initial_state)\n",
    "    print(' << lstm_outputs >>', lstm_outputs)\n",
    "    seq_output_reshaped = tf.reshape(lstm_outputs,shape=[-1, self.lstm_size],name='seq_output_reshaped')\n",
    "    logits = tf.layers.dense(inputs=seq_output_reshaped,units=self.num_classes,activation=None,name='logits')\n",
    "    proba = tf.nn.softmax(logits,name='probabilities')\n",
    "    y_reshaped = tf.reshape(y_onehot,shape=[-1, self.num_classes],name='y_reshaped')\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y_reshaped),name='cost')\n",
    "    # Gradient clipping to avoid \"exploding gradients\"\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),self.grad_clip)\n",
    "    optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "    train_op = optimizer.apply_gradients(zip(grads, tvars),name='train_op')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "132db161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, train_x, train_y,num_epochs, ckpt_dir='./model/'):\n",
    "    ## Create the checkpoint directory\n",
    "    ## if it does not exists\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.mkdir(ckpt_dir)\n",
    "    with tf.Session(graph=self.g) as sess:\n",
    "        sess.run(self.init_op)\n",
    "        n_batches = int(train_x.shape[1]/self.num_steps)\n",
    "        iterations = n_batches * num_epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            # Train network\n",
    "            new_state = sess.run(self.initial_state)\n",
    "            loss = 0\n",
    "            ## Mini-batch generator:\n",
    "            bgen = create_batch_generator(\n",
    "            train_x, train_y, self.num_steps)\n",
    "            for b, (batch_x, batch_y) in enumerate(bgen, 1):\n",
    "                iteration = epoch*n_batches + b\n",
    "                feed = {'tf_x:0': batch_x,'tf_y:0': batch_y,'tf_keepprob:0' : self.keep_prob,self.initial_state : new_state}\n",
    "                batch_cost, _, new_state = sess.run(['cost:0', 'train_op',self.final_state],feed_dict=feed)\n",
    "                if iteration % 10 == 0:\n",
    "                    print('Epoch %d/%d Iteration %d | Training loss: %.4f' % (epoch + 1, num_epochs,iteration, batch_cost))\n",
    "            ## Save the trained model\n",
    "            self.saver.save(sess, os.path.join(ckpt_dir, 'language_modeling.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e08ec9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(self, output_length,ckpt_dir, starter_seq=\"The \"):\n",
    "    observed_seq = [ch for ch in starter_seq]\n",
    "    with tf.Session(graph=self.g) as sess:\n",
    "        self.saver.restore(sess,tf.train.latest_checkpoint(ckpt_dir))\n",
    "        ## 1: run the model using the starter sequence\n",
    "        new_state = sess.run(self.initial_state)\n",
    "        for ch in starter_seq:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = char2int[ch]\n",
    "            feed = {'tf_x:0': x,'tf_keepprob:0': 1.0,self.initial_state: new_state}\n",
    "            proba, new_state = sess.run(['probabilities:0', self.final_state],feed_dict=feed)\n",
    "        ch_id = get_top_char(proba, len(chars))\n",
    "        observed_seq.append(int2char[ch_id])\n",
    "\n",
    "        ## 2: run the model using the updated observed_seq\n",
    "        for i in range(output_length):\n",
    "            x[0,0] = ch_id\n",
    "            feed = {'tf_x:0': x,'tf_keepprob:0': 1.0,self.initial_state: new_state}\n",
    "            proba, new_state = sess.run(['probabilities:0', self.final_state],feed_dict=feed)\n",
    "            ch_id = get_top_char(proba, len(chars))\n",
    "            observed_seq.append(int2char[ch_id])\n",
    "    return ''.join(observed_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad06c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_char(probas, char_size, top_n=5):\n",
    "    p = np.squeeze(probas)\n",
    "    p[np.argsort(p)[:-top_n]] = 0.0\n",
    "    p = p / np.sum(p)\n",
    "    ch_id = np.random.choice(char_size, 1, p=p)[0]\n",
    "    return ch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bfabc106",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CharRNN' object has no attribute 'build'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-f3f793253187>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreshape_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_ints\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCharRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./model-100/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-dcbe8fb175f1>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_classes, batch_size, num_steps, lstm_size, num_layers, learning_rate, keep_prob, grad_clip, sampling)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m123\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msampling\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CharRNN' object has no attribute 'build'"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_steps = 100\n",
    "train_x, train_y = reshape_data(text_ints,batch_size,num_steps)\n",
    "rnn = CharRNN(num_classes=len(chars), batch_size=batch_size)\n",
    "rnn.train(train_x, train_y,num_epochs=100,ckpt_dir='./model-100/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "404a7605",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-536777f59bf1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m123\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCharRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./model-100/'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rnn' is not defined"
     ]
    }
   ],
   "source": [
    "del rnn\n",
    "np.random.seed(123)\n",
    "rnn = CharRNN(len(chars), sampling=True)\n",
    "print(rnn.sample(ckpt_dir='./model-100/',output_length=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1698ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
